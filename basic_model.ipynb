{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "drive_folder = '/content/drive/Shareddrives/682_Drive'\n",
    "# Adjust this line to be the assignment1 folder in your google drive\n",
    "notebook_folder = drive_folder + '/682-Project'\n",
    "%cd {notebook_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Hugging Face API key and model\n",
    "HF_API_KEY = '<your_hf_api_key>'\n",
    "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"\n",
    "API_URL = f\"https://api-inference.huggingface.co/models/{MODEL_ID}\"\n",
    "\n",
    "# File paths\n",
    "input_file_path = '/content/drive/Shareddrives/682_Drive/682-Project/ag_news_train_imbalanced.csv'\n",
    "output_file_path = '/content/drive/Shareddrives/682_Drive/682-Project/ag_news_with_LLM_augmented_world_new_approach.csv'\n",
    "random_samples_path = '/content/drive/Shareddrives/682_Drive/682-Project/random_samples_new_approach.csv'\n",
    "augmented_samples_path = '/content/drive/Shareddrives/682_Drive/682-Project/augmented_samples_new_approach.csv'\n",
    "\n",
    "# Load the dataset\n",
    "imbalanced_dataset = pd.read_csv(input_file_path)\n",
    "world_data = imbalanced_dataset[imbalanced_dataset['label'] == 0]\n",
    "\n",
    "# Set the number of augmented samples to generate\n",
    "total_augmented_samples = 20\n",
    "batch_size = 10\n",
    "num_batches = total_augmented_samples // batch_size\n",
    "\n",
    "# Step 1: Randomly select samples from the \"World News\" class\n",
    "random_samples = world_data.sample(n=5, random_state=42)['text'].tolist()\n",
    "\n",
    "\n",
    "base_prompt = (\n",
    "    \"Generate 10 new detailed, unique, and high-quality news articles for the 'World News' category which are similar to the below samples. \"\n",
    "\n",
    ")\n",
    "\n",
    "for i, sample in enumerate(random_samples):\n",
    "    base_prompt += f\"Example {i+1}: {sample}\\n\\n\"\n",
    "\n",
    "print(base_prompt )\n",
    "#base_prompt += (\n",
    "    #\"\\n Generate articles after 'output' \\nOutput:\\n\"\n",
    "    #\"Now, write 10 new articles in the same style and structure as the examples. Each article should be between 3 to 5 sentences and focus on recent global events. \"\n",
    "    # \"Clearly separate the articles as follows:\\n\\n\"\n",
    "    # \"Article 1:\\n[Start writing here...]\\n\\n\"\n",
    "    # \"Article 2:\\n[Start writing here...]\\n\\n\"\n",
    "    # \"Article 3:\\n[Start writing here...]\\n\\n\"\n",
    "    # \"Article 4:\\n[Start writing here...]\\n\\n\"\n",
    "    # \"Article 5:\\n[Start writing here...]\\n\\n\"\n",
    "    # \"Article 6:\\n[Start writing here...]\\n\\n\"\n",
    "    # \"Article 7:\\n[Start writing here...]\\n\\n\"\n",
    "    # \"Article 8:\\n[Start writing here...]\\n\\n\"\n",
    "    # \"Article 9:\\n[Start writing here...]\\n\\n\"\n",
    "    # \"Article 10:\\n[Start writing here...]\\n\\n\"\n",
    "#)\n",
    "\n",
    "# Processing Generated Output\n",
    "def split_articles(generated_texts):\n",
    "    articles = []\n",
    "    for text in generated_texts:\n",
    "        parts = text.split(\"Example\")\n",
    "        for part in parts:\n",
    "            article = part.strip()\n",
    "            if article and \":\" not in article:\n",
    "                articles.append(article)\n",
    "    return articles\n",
    "\n",
    "#print(base_prompt)\n",
    "# Function to call the Hugging Face API for text generation\n",
    "def generate_augmented_samples_with_flan_t5(prompt, num_samples, batch_size):\n",
    "    all_generated_texts = []\n",
    "    headers = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}\n",
    "\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": 300,  # Adjust max tokens per article\n",
    "            \"temperature\": 0.8,\n",
    "            \"top_k\": 50,\n",
    "            \"top_p\": 0.9,\n",
    "        },\n",
    "    }\n",
    "    response = requests.post(API_URL, json=payload, headers=headers)\n",
    "\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        output_text = response.json()[0]['generated_text']\n",
    "        print(output_text)\n",
    "        all_generated_texts.append(output_text)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "    return all_generated_texts\n",
    "\n",
    "# Step 3: Generate augmented texts in batches\n",
    "all_augmented_texts = []\n",
    "for batch_num in tqdm(range(num_batches), desc=f\"Generating batches with {MODEL_ID}\"):\n",
    "    batch_augmented_texts = generate_augmented_samples_with_flan_t5(base_prompt, num_samples=batch_size, batch_size=batch_size)\n",
    "    all_augmented_texts.extend(batch_augmented_texts)\n",
    "\n",
    "\n",
    "pd.DataFrame({'text': random_samples}).to_csv(random_samples_path, index=False)\n",
    "\n",
    "# Split the generated text into individual articles\n",
    "augmented_articles = split_articles(all_augmented_texts)\n",
    "\n",
    "# Step 5: Save augmented samples in CSV\n",
    "augmented_df = pd.DataFrame({'text': augmented_articles[:total_augmented_samples], 'label': 0})\n",
    "augmented_df.to_csv(augmented_samples_path, index=False)\n",
    "\n",
    "# Step 6: Append the augmented texts to the original dataset\n",
    "combined_world_data = pd.concat([world_data, augmented_df]).reset_index(drop=True)\n",
    "combined_world_data.to_csv(output_file_path, mode='a', header=not os.path.exists(output_file_path), index=False)\n",
    "\n",
    "print(f\"Successfully augmented {len(augmented_articles[:total_augmented_samples])} texts for the 'World News' class.\")\n",
    "\n",
    "# Step 7: Load the augmented data and combine it with other classes\n",
    "augmented_world_data = pd.read_csv(output_file_path)\n",
    "sports_data = imbalanced_dataset[imbalanced_dataset['label'] == 1]\n",
    "business_data = imbalanced_dataset[imbalanced_dataset['label'] == 2]\n",
    "sci_tech_data = imbalanced_dataset[imbalanced_dataset['label'] == 3]\n",
    "\n",
    "# Combine and shuffle the dataset, then save to CSV\n",
    "final_data = pd.concat([augmented_world_data, sports_data, business_data, sci_tech_data]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "final_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"Class distribution in the augmented dataset:\")\n",
    "print(final_data['label'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM1Rh3YXoY993IkCTVesoUj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
